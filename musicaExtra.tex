\chapter{Música digital em domínios não digitais}
\label{cap:musicaExtra}

As tecnologias expostas neste trabalho tiveram origem em
usos reais, advindas
de práticas culturais e voltadas para repercussões sociais.
Este material humano e artístico é o assunto deste
capítulo.
Pode-se dividir estes resultados em:
experimentos abertos em áudio,
música em tempo real,
música em tempo diferido,
música na matéria e
repercussões no tecido social.
Estes tópicos são tratados separadamente
abaixo em termos do que é
mantidas \emph{online}.
Estas atividades extrapolam os usos musicais, especialmente devido às
finalidades pedagódicas e sociais dos materiais audiovisuais.
Assim, por completude, alguns materiais didáticos
e ferramentas web são descritas.

\section{Experimentos abertos em áudio: LADSPAs, Wavelets e Redes Complexas}

Todos os desenvolvimentos desta dissertação estão em repositórios
abertos.\cite{repoDissertacao}

O código computacional feito para a arte sonora, incluindo a musical,
 manifesta-se como cultura pois é fruto de práticas
espontâneas, diárias e coletivas.
\footnote{As chamadas culturas biopunk, ciberpunk, cipherpunk, hacker,
  digital e outras mais, possibilitadas pelos desenvolvimentos
  em telecomunicação, dizem respeito em menor ou maior grau à produção
  de código como cultura. As elaborações de conceitos e ferramentas
  voltados para o compartilhamento nestas culturas estão na gênese do que
  se entende como 'Cultura Livre'.}
A seguir estão alguns exemplos de resultados
destas incidências culturais,
especificamente em áudio
 e música.

\subsubsection{Plugins LADSPA e lv2}

LADSPA (Linux Audio Developers Simple Plugin API) é a API
livre de plugins de áudio até a presente data. A
última versão é a 1.1, em uso corrente até hoje e é
de 2002 segundo os arquivos de cabeçalho dos códigos relacionados.
O LADSPA \emph{version} 2, abreviado lv2, é uma segunda versão do
protocolo também estabelecida e estável, mas extensível, motivo pelo
qual está sempre em desenvolvimento.

Na síntese, para maior qualidade, 
pode-se fazer a recomposição a partir do espectro desejado, como na seção~\ref{subsec:ruidos}.
Menos purista porém mais eficiente é
usar uma amostra curta do ruído e reproduzi-la indefinidamente
através de \emph{cross-fades}. Esta eficiência é desejada em um plugin,
pois seus usos comuns estão aplicações em tempo real.

Através deste trabalho, foi disponibilizado um pacote de plugins lv2
que sintetizam os ruídos branco, azul, violeta, rosa e marrom. Os
  códigos em C e a implementação como plugin lv2 estão no
  repositório git como pode ser visto em \emph{online}\footnote{
      este link é o README do repositódio git com todos os códigos dos
      plugins:
  http://labmacambira.git.sourceforge.net/git/gitweb-index.cgi.}.

Já na remoção de ruídos as abordagens são as mais variadas e
extremamente dependentes da aplicação. A complexidade dos algorítmos
atinge níveis de especialidade em processamento de sinais que já
mereceram trabalhos dedicados. Adiante está a implementação de um
removedor de ruído \emph{'Hum'}. Este ruído é
geralemte causado pela corrente alternada que
alimenta os equipamentos utilizados.

A remoção de ruído \emph{Hum} é baseada em uma sequência de filtros nó rejeita
banda (veja subseção~\ref{subsec:filtros}
idealmente dispostos nos harmônicos da frequência de
oscilação da corrente elétrica. Cada um destes filtros deve permitir
ajustes finos no fator de qualidade e também na frequência central
pois trata-se de um sistema real passível dos mais diversos efeitos de
distorção do modelo ideal. O código C/C++ e a
implementação como plugin está no mesmo repositório disponibilizado
para a síntese de ruídos.


\subsubsection{\emph{Wavelets}}
Alguns dos experimentos em \emph{wavelets} praticados estão em
Audioexperiments.\cite{Audioexperiments} Uma destas investidas
superou em muito um experimento e constitui um protocolo de compactação de áudio desenvolvido com o Prof. Rafael Santos Mendes na FEEC/UNICAMP como consequência do mestrado realizado por André Luvizzoto sob orientação do mesmo professor.
O método está bem descrito no artigo desenvolvido. Em resumo, o método consiste particionar o áudio com sobreposições para contemplar aplicações reais sem as distorções de borda da decomposição e recomposição. Em seguida, o áudio é decomposto em árvore \emph{Wavelet Packet}. Cada folha é então ordenada, os coeficientes com baixa energia eliminadose um polinômio encontrado pelo método dos mínimos quadrados é usado para representar os coeficientes restantes. Detalhes de escrita e leitura do polinômio completam o protocolo. Aplicações para stream de voz em tempo real foram vislumbradas e testes adicionais propostos.\cite{FabbriWavelets}

\subsubsection{Redes Complexas}
Com o fim de detectar nuances emocionais no uso da linguagem natural, foram desenvidos alguns trabalhos em análise de fala e de texto, ambos focados no uso de redes complexas. Três trabalhos: dois de processamento de texto e um de fala. O primeiro trabalho de processamento de texto foi apresentado na ACL de 2010, em Upsala, na Suécia, pelo Prof. Thiago Pardo, atual presidente da ACL.\cite{FabbriACL,FabbriComplenetVoz,FabbriComplenetTexto}

Os trabalhos focaram em distinguir polaridade em excertos de textos e de fala. Nos textos, a rede é formada a partir da sequência de palavras, em que cada palavra é um vértice e a sucessão imediata cria uma aresta. O pré-processamento dos textos consiste na lematização e na retiradas das \emph{stop-words} e da pontuação. Nas falas, as bandas de frequência utilizadas são vértices e a transição entre uma banda e outra é uma aresta. Ambas redes com peso e podem ser utilizadas como direcionais ou não. Os trabalhos consistiam em formar estas redes com os bancos de dados correlatos, extrair medidas e aplicar reconhecimento de padrão. Ambos os estudos não fazem uso do conteúdo semântico, mas sim topológico dos sinais recebidos, e chegam a mais 80\% de cobertura ou precisão em alguns casos, podendo contribuir com métodos tradicionais que fazem uso de redes semânticas, por exemplo.

\section{Áudio e Música}

Este capítulo está dividido em 4 partes: música em
tempo diferido (ou seja, que não é feito em tempo real), música em
tempo real, música na matéria (suporte físico em \emph{hardware}) e música no
tecido social (considerando especialmente as mobilizações humanas
relacionadas).

Com isso desejamos expor uma incidência da prática musical através do código com
exemplos reais de aplicação e em utilização pelo autor, membros do
LabMacambira.sf.net, parceiros, colaboradores e por usuários
eventuais das naturezas mais diversas.


\subsection{Música em Tempo Diferido: Minimum-fi e FIGGUS}

\begin{quotation}
\small 'I also find that intelligent people always respect the intelligence needed to construct a simple structure in a clear way that really works.'

\emph{Tom Johnson}
\end{quotation}

A realização musical em tempo diferido é o paradigma inicial da música
computacional, iniciando com o Music V, depois
com o CSound. Pode-se dizer que até hoje é a forma como
compositores usualmente pensam a música: concebendo e escrevendo as
estruturas que depois são executadas por instrumentistas ou aparelhos
eletrônicos.

Assim como a composição instrumental permite alguns refinamentos estruturais não presentes na improvisação instrumental,
a realização musical em
tempo diferido permite um detalhamento maior dos
procedimentos do que a realização em tempo real. Por este mesmo
motivo.

O som musical pode ser caracterizado fisicamente, tornando possível a  sua síntese
digital. Estes sons e a organização deles são objeto do Capítulo 2 da presente dissertação.
A utilização destes sons diretamente, utilizando somente os recursos básicos de justaposição e sobreposição, está na pequena peça \emph{minimum-fi}. Já o \figgus\ (FInite Groups in Granular and Unit
Synthesis), utiliza os princípios do minimum-fi e utiliza simetrias, através de
permutações e de Teoria de Grupos, para a composição de músicas.
O \figgus\ gera um
EP\footnote{\emph{Extended Play}, um álbum musical maior que um single mas
  menor que um LP (Long Play) inteiro.} com um único comando. Este é o
\emph{PPEPPS}\footnote{Pure Python EP: Projeto Solvente}, como veremos
a seguir.

Desta forma, esta sessão exemplifica e explicita - através de dois
exemplos reais - o uso de código para a síntese
\emph{musical}, desde as amostras relativas a uma nota com dada
frequencia, amplitude e timbre, até a confecção de uma ferramenta
derivada, já incorporando propostas musicais e estruturas mais
elaboradas.

\subsubsection{\emph{Minimum-fi}}

Existe uma perene matiz estética e também tecnológica dedicada a realizar uma
dada tarefa com \emph{o mínimo} necessário.  Na música, esta matiz é
igualmente presente e decorre em grande parte do princípio de unidade
e coerência\footnote{Este princípio tanto é fundamental que as escolas
  musicais possuem técnicas específicas, músicas possuem suas próprias
  convenções mantidas por toda a sua duração, os arcos mantém
  características, enfim, podemos até mesmo concluir que as simetrias
  definem o escopo musical.}. 
  Em código computacional, a empreitada para
manifestar este princípio ele próprio de forma mínima resultou no
\emph{minimum-fi.py}, código Python em um único arquivo curto que
sintetiza a música segundo as estruturas especificadas em
linha. Na versão atual, de
2012 mas adiantada em 2011, os algorítmos em Python propriamente ditos
somam 53 linhas e incluem apenas 5 funções. Com estas funções,
estruturas musicais são criadas padrão a padrão, nota a nota,
amostra por amostra. Na prática, e em linguagem cotidiana,
as notas formam blocos e estruturas hierarquicamente
superiores.

Os princípios, bastante simples\footnote{Osvaldo Lacerda, em seu livro
  \emph{Compêndio de Teoria Elementar da Música} fala das propriedades
  do som musical e sua organização de forma condizente.}, são:
\begin{itemize}
  \item Deve-se ter um mecanismo de síntese sonora que possibilite a
    geração de unidades sonoras com diferentes timbres, controle sobre
    a frequência fundamental, duração e volume, como especificado na seção~\ref{sec:notasDisc}.
  \item Deve-se ser capaz de construir
    séries de unidades, sejam sobrepostas (e.g. acordes)
    ou justapostas (e.g. melodias).
\end{itemize}

Para o primeiro item, se prestam os procedimentos de busca
em tabelas/vetores com formas de ondas em alta resolução, chamado
\emph{lookup table}. O procedimento é barato computacionalmente,
com resultados diversificados e tidos como de alta qualidade
pois não acrescentam ruídos relevantes ao sinal. Uma descrição
do procedimento está na subseção~\ref{subsec:lookup}.

Através da utilização do lookup sucessivo (procura-se um valor na primeira tabela
e este valor indica o valor na segunda tabela a ser utilizado), executa-se
um \emph{waveshaping}. Este procedimento é bastante apreciado pela simplicidade e eficácia
na síntese de tímbres diversos e ricos em harmônicos e evolução temporal. Embora
uma explicação exaustiva do waveshaping fuja ao escopo deste trabalho, este
método se caracteriza pela aplicação de uma função não linear ao sinal de entrada.
Neste caso o sinal de entrada é gerado pela primeira busca, a função não linear aplicada
é a segunda busca, na outra tabela.

Existem muitas formas de se executar waveshaping, a seguir segue o que usamos no minimum-fi, que se sustenta principalmente por ser leve e simples:

\code{Waveshaping com consultas sucessivas a tabelas}{python_snippets/lookup_cruz.py}

O segundo item - dos dois princípios expostos sobre o minimum-fi - presta-se à discretização do espaço musical.
Unidades como batidas e notas
tornam mais eficiente a comunicação pois a quantidade
de estruturas sugeridas é maior e as estruturas são mais claras no discreto do que no contínuo.\cite{Roederer} Roederer chega a
apontar que as próprias notas dos instrumentos musicais são um reflexo de que é mais eficiente
o uso do discreto do que do contínuo para a geração de estruturas musicais.

De fato, unidades bem definidas se mostram úteis na prática musical 
para fazer sequências de unidades, concatená-las no tempo. Quando as unidades
são notas com frequência definida, as sequências de unidades justapostas no tempo tendem a ser compreendidas como melodias ou linhas melódicas. As
sequências sobrepostas no tempo são comumente compreendidas como acordes, mas podem ser tidas simplesmente
como sobreposições circunstanciais de duas ou mais linhas melódicas\footnote{A música do século XX apresentou diversos modelos teóricos que quebram com este entendimento simplificado sobre a música, suas unidades básicas e estruturas relacionadas}.\cite{Lacerda}

As duas construções básicas explicitadas, baseadas na dicotomia melodia/harmonia - relacionada à horizontalidade/verticalidade, justaposição/sobreposição - são
as funções \emph{fazSequencia} e \emph{fazAcorde} no minimum-fi. Vale notar elas são absolutamente 
equivalentes em uma análise puramente conceitual, i.e. uma delas pode ser omitida sem perda das possibilidades musicais. Isso fica particularmente óbvio quando se nota que os procedimentos de mixagem e concatenação são
plenamente capazes de realizar o que estas funções realizam. Aliás, as funções nada
mais são do que usos típicos e quase caricatos destes procedimentos: no \emph{fazAcorde} a mixagem
sobrepõe no tempo todas as unidades, no \emph{fazSequencia} as unidades são todas juntapostas no tempo.

Como pode-se notar a seguir, as sequências de notas e os acordes, em última instância, são utilizações específicas das 2 funções de síntese sonora explicadas anteriormente: lookup e lookupcruz.

\code{Realização de Sequências de Notas}{python_snippets/fazSequencia.py}

\code{Realização de Acordes de Notas}{python_snippets/fazAcorde.py}

A última das cinco funções utilizadas é uma soma amostra a amostra de dois sons. Para isso,
é necessário completar com zeros a sequência com o menor número de amostras para somar rapidamente:

\code{Somador (função auxiliar)}{python_snippets/somador.py}

Com isso o básico está coberto e o foco vai para a criação de estruturas. Por exemplo: 
pode-se criar as
escalas completamente simétricas na oitava cromática, escalas diatônicas ou até microtonais, como as descritas na subseção~\ref{subsec:intervalos}.

O estabelecimento de pequenas sequências é bastante útil para
reutilizações, variações e geração de materiais derivados, por exemplo: 
\code{Sequências diversas}{python_snippets/series.py}

O passo seguinte é sintetizar e mixar para a obtenção 
de sequências musicais. A síntese de sequências e acordes
podem ser feitos desta forma:

\code{Sintetizando sequências e acordes}{python_snippets/seqs_acordes.py}

Já a síntese de estruturas compostas (e.g. sequências de acordes e sobreposição
de linhas melódicas), é feita com os recursos usuais da liguagem:
listas em Python e vetores Numpy (mais eficientes em tempo de
execussão e também na simplicidade do código). Estes mesmos
procedimentos são praticamente os mesmos em Scilab, C/C++, Javascript, PHP, etc.

A seguir, para fins didáticos, está a construção de acordes periódicos em
python puro\footnote{Uma implementação não didática destas três linhas de código
pode ser feita em uma só linha.}:

\code{Acordes periódicos}{python_snippets/acordes_periodicos.py}

Em posse destas sequências, acordes e recursos da linguagem,
são formadas estruturas hierarquicamente superiores
através da concatenação de estruturas, da mixagem de estruturas, e da amplificação
(ou atenuação) seletiva das mesmas:

\code{Amplificação e mixagem}{python_snippets/amp_mix.py}

Neste ponto, basta criar músicas e sequências de interesse estético ou para pesquisa. Os encadeamentos dependem de intensões estéticas, entendimentos musicais e estruturas abstratas que mantém a coerência e o interesse em uma peça musical. Para o leitor mais interessado, recomendamos uma visita ao capítulo~\ref{cap:resultados} e à \emph{toolbox} \massa, onde pode-se encontrar o próprio script \emph{minimum-fi.py} e outros experimentos. 
A seguir utilizamos esta base apresentada para sintetizar estruturas musicais e então um EP.

\vspace{10 mm}

\subsubsection{FIGGUS: FInite Groups in Granular and Unit Synthesis}

Nesta subseção há um foco especial nas estruturas musicais.
São elas, inseridas em um momento histórico e
executadas por instrumentos específicos com as técnicas de época,
que constituem em grande parte uma linguagem musical e músicas propriamente
ditas. O \figgus\ constitui uma técnica composicional
manifesta em software como ferramenta de síntese de
estruturas musicais. Esta técnica
consiste na utilização de estruturas matemáticas
para a representação de simetrias.

O \figgus\ foi iniciado em 2006 com o físico-matemático Prof. Adolfo Maia Junior (do IMECC e NICS, ambos da UNICAMP) - bem anterior
ao nascimento do \emph{minimum-fi} - para
tratar de simetrias na música com vistas à composição musical através
de métodos matemáticos\footnote{Duas iniciações científicas trataram do assunto.}. Mais especificamente, a proposta resultou em
um programa voltado para a síntese
granular e síntese de estruturas musicais através de Grupos Algébricos. O nome dado
foi \figgus, sigla de FInite Groups in Granular and Unit Synthesis\footnote{Também foi usado
o nome FIGGS (FInite Group in Granular Synthesis) dado que o termo \emph{unit synthesis} não
é usual na literatura. Posteriormente o primeiro autor deste trabalho recorreu novamente
ao uso do nome \figgus. Isso foi motivado pelo
uso da técnica para síntese de estruturas musicais, não para
síntese de amálgamas sonoros (ou timbres mesmo) tipicamente resultantes da síntese granular}.

Na atual reescrita, embora ainda bastante atrás do \figgus\ original quanto
à interface gráfica, a ferramenta opera diretamente em Python puro,
com as biblitecas imbutidas por padrão. Isso permite com que o FIGGUS
sintetize todo um EP usando somente os comandos:

\code{Utilizando o FIGGUS para Sintetizar um EP}{python_snippets/synth_ep.py}

Desta forma, a ferramenta 
é simples de ser usada para experimentações
e implementações adicionais.
Outro uso planejado para o \figgus\
é a síntese de tímbres e amálgamas sonoros através da
Síntese Granular. Embora o foco atual seja outro, cabe algumas breves
palavras sobre o assunto.

A Síntese Granular é uma área bem estabelecida tanto na acústica quanto
na Computação Musical e se caracteriza pela geração de sons bastante curtos
e em quantidade massiva. Tipicamente, os sons possuem entre 5 e 40 milissegundos
e a quantidade destes \emph{microsons}\footnote{Grãos sonoros e microsons são jargões equivalentes
típicos da síntese granular. São usados para indicar sons com durações
bastante curtas, como assinalado no texto.} pode chegar a milhares por segundo. O tratamento específico da
síntese granular foge ao escopo deste trabalho. O leitor interessado pode consultar os artigos produzidos sobre Síntese Granular e Teoria de Grupos.\cite{figgusOriginal,figgusEspacializacao}
Desta forma, o texto a seguir concentra-se em Grupos Finitos para a síntese de estruturas musicais.

Nas artes é de comum conhecimento o papel absolutamente central
que as simetrias possuem. Na música, para citar somente alguns exemplos simples,
há os numerosos estudos de simetrias na música de J. S. Bach, os jogos
de dados de Mozart e os usos recorrentes da proporção áurea na música
de Béla Bártok. Matematicamente, as \emph{simetrias} são descritas por Grupos,
e estes são definidos como um conjunto (seja $G$)
munido de uma operação (seja $\bullet$), formando um grupo $(G,\bullet)$
satisfazendo as propriedades descritas na subseção~\ref{estCic}.

No \figgus, o grão ou unidade sonora
é uma classe que possui apenas
os seguintes atributos: duração (segundos),
frequência (Hz), timbre (identificador para usar mediante implementações convenientes), intensidade (pico $\in \ [0,1]$), e duração dos fades (in e out em segundos).

%\code{Grão Sonoro Básico}{python_snippets/fgrao.py}

O \figgus\ funciona com base em uma sequência de grãos especificada inicialmente e na qual operam as permutações.

%\code{Sequência de Grãos}{python_snippets/fsequencia.py}

Aos grãos em sequência são aplicadas permutações.
Para isso é bastante conveniente representar as permutações em classes próprias.
A classe do padrão de permutação possui também um período de aplicação da permutação, ou seja, de quantas em quantas leituras da sequência apermutação é aplicada.

%\code{Permutações e Padrões de Permutações}{python_snippets/fpermutacoes.py}

De posse dos grãos, da sequência, das permutações e do padrão de permutações, pode-se realizar a estrutura musical em si. Basta adicionalmente especificar o número desejado de iterações da sequência. Com isso, a sequência de grãos é lida um número de vezes, aplicando as permutações na sequência de grãos segundo o padrão especificado, de forma a resultar em uma sequência musical. Note que se a permutação usar menos elementos que a sequência possui, alguns destes elementos ficarão estáticos nas iterações da sequência no padrão sonoro.

%\code{Realização do Padrão Musical}{python_snippets/fpadraosonoro.py}

Em posse não apenas de representações abstratas do padrão musical a ser realizado, mas dos próprios vetores sonoros relacionados à representação digital da música a ser realizada, pode-se escrever um arquivo de áudio propriamente dito. O mais conveniente neste caso é escrever um arquivo PCM (Pulse Code Modulation) em algum padrão amplamente utilizado e reconhecido. Ambos WAV e AIFF satisfazem estes requisitos. Mais especificamente, o padrão de CDs é WAV com 44100 amostras por segundo e 16 bits por amostra. As amostras dos vetores sonoros são calculados, por conveniência e convenção, no âmbito $[-1,1]$ e precisam ser normalizados para o âmbito $[-32767,32768]$ e truncados em números inteiros. Depois disso devem ser escritos em um arquivo com os \emph{bits} como na convenção da linguagem C/C++. A bibioteca \emph{struct} cuida dessa escrita do inteiro no formato correto, e a biblioteca \emph{wave} escreve o cabeçalho no formato WAV adequado. Assim, a clase de escrita do vetor sonoro em arquivo comum fica assim:

\code{Escrita do Vetor Sonoro em Arquivo WAV}{python_snippets/fio.py}


\subsection{Música em Tempo Real: Live coding e ABeatTracker (ABT)}

Tornou-se usual a síntese
sonora em tempo real mesmo em \emph{laptops} populares.
Assim, surgiram linguagens de
domínio específico, para o áudio e a música, em sua maioria dedicadas
- ou ao menos capacitadas - para interação em tempo de
execução como \emph{Puredata}, \emph{SuperCollider}, \emph{ChucK}
 e \emph{ixilang}. Todos estes são exemplos de
linguagens largamente utilizadas para a composição musical e síntese
sonora em tempo real. Em outras palavras, estas linguagens
possibilitam que o usuário ouça o resultante sonoro do código
utilizado e altere o código com resultados imediatos no processamento e
sonoridades produzidas.

A exploração estética destas ferramentas 
em performances musicais é a proposta do \emph{Live coding}.
Esta prática de \emph{performance} se dedica
especialmente à escrita de código em
tempo real com vistas à projeção visual dos códigos enquanto
são escritos, junto à projeção sonora que produzem.
O ABeatTracker ainda pode ser considerado
livecoding, mas já é um intermediário, com traços
de um aplicativo ou ferramenta, embora os comandos sejam
pela escrita. É uma linguagem
com macros para execução de rítmos através de \emph{samples}
em conjunto com instrumentos
tradicionais e outras fontes sonoras/musicais externas.

\subsubsection{Live coding}

Recentemente, grupos de ponta em música experimental
estão desenvolvendo apresentações musicais públicas baseadas na
escrita de código ao vivo. Este é um fenômeno cultural e
estético, do qual vale ressaltar os grupos pioneiros
\emph{Slub} e \emph{Benoît and the Mandelbrots} assim como
as 'orquestras de \emph{laptops}'
\emph{PLOrk}, \emph{SLOrk} e \emph{DubLOrk}\footnote{Para maiores detalhes, veja:
  \url{http://toplap.org}.}. Usualmente, o código é projetado para que a
audiência possa ver o que está sendo escrito, no rítmo em que se
escreve, e se projeta também o resultante sonoro por autofalantes.



Estas são motivações presentes em diferentes
grupos\footnote{Vide 'Manifesto Live coding' em
  \url{http://toplap.org/wiki/ManifestoDraft}.}, embora não
necessariamente ou da mesmícima forma:

\begin{itemize}
    \item A apresentação musical, com o uso do computador como instrumento,
    carece de recursos
      performáticos visuais dos instrumentos tradicionais.
       Os gestos são por demais discretos e a
      concentração do \emph{performer}/instrumentista é bastante focada na tela do
      computador.
    \item O \emph{feedback} auditivo do código projetado permite que o
      espectador infira significados dos códigos. 
      Este recurso do \emph{livecoding} é usado para
      desmistificar a programação de computadores e sua aplicação em
      computação musical, comumente considerada
      intangível.
    \item O código em si é um recurso poderosíssimo que permite ao
      usuário controlar os sons produzidos amostra por amostra ou em
      escalas maiores de tempo, como notas, compassos, fraseados inteiros ou
      mesmo em escalas maiores de tempo, como minutos, horas, dias e
      semanas.
    \item O compartilhamento do código é usual, leve e eficiente como
      entrega de tecnologia valiosa para a
      proposta estética. Isso motiva não só o programador a aplicar
      seus conhecimentos na música, mas também o músico a adentrar o
      uso de linguagens de programação para expressão de suas ideias
      musicais.
\end{itemize}

Desta forma, foi iniciada em 2011 uma linha de atuação em \emph{livecoding} com
a criação do duo \emph{FooBar} composto por Vilson Vieira e o autor
deste escrito. Este duo se desenvolveu no
trio \emph{FooBarBaz}/Variáveis Metasintáticas composto
pelo duo e por Gilson Beck na mesa de som, usando detecção de cor no laptop
para incrementar a apresentação. Este trio
realizou uma \emph{performance} no \emph{V Festival Contato}.
 É interessante ressaltar que, até onde se sabe,
essa foi a primeira apresentação de \emph{livecoding} no Brasil, e a maior
apresentação internacional em tamanho de platéia, estima-se que entre 3 e 5 mil
pessoas estavam presentes.

Foi usada a linguagem emph{ChucK} por apresentar os recursos que
o duo inicial considerou mais apropriados,
 embora de forma alguma isso seja
consensual na prática atual de \emph{livecoding}. Além disso, a apresentação
contou com recursos adicionais para agregar interesse, como a
utilização do \emph{cowsay} para enviar mensagens enquanto se
desenrolava a música. Em especial, as trajetórias de um ponto branco
no fundo do código sugeria o sono REM como uma experiência coletiva de alteração
do estado.
Estes recursos podem ser vistos
em uso nos videos demonstrativos\footnote{Disponíveis em
  \url{http://vimeo.com/33012735}, \url{http://vimeo.com/33018740},
  \url{http://vimeo.com/33019291}, \url{http://vimeo.com/33025717} e
  \url{http://vimeo.com/33025913}.}
  As linhas do \emph{cowsay} e o
\emph{script} em linguagem Processing relacionados, assim como maiores
detalhes sobre o duo e fotos da apresentação podem ser vistos em
\url{http://wiki.nosdigitais.teia.org.br/FooBarBaz}. Estavam presentes
no palco outros membros do labMacambira.sourceforge.net: Ricardo Fabbri,
Alexandre 'Bzum', Larissa.

Especificamente sobre a prática do \emph{livecoding},
duas pessoas executaram \emph{scripts} em \emph{Chuck} simultaneamente.
A saber, Vilson
Vieira executou ritmos, batidas bastante marcadas que serviam como
base. Renato Fabbri, autor deste texto, executava linhas
fluídas quasi-melódicas que formavam arcos maiores.
Havia também interlúdios em que ambos se
revesavam com músicas curtas e inusitadas, como em um duelo. A mixagem
e espacialização dos dois canais de áudio foram controladas por um
patch em Puredata que permitia o \textit{cross-fading} entre os canais
através da detecção do movimento das mãos. Isso foi possível através
do objeto em Puredata/GEM chamado \emph{color classify}, criado por
Ricardo Fabbri e posto em uso por Gilson Beck durante a
apresentação. Esse mesmo objeto foi utilizado no instrumento
AirHackTable, discutido na seção~\ref{aht}.

A prática de \emph{livecoding} requer o uso de 
instruções curtas, incentivando o improviso
musical através do código. Ambos \emph{livecoders}
usaram bons editores de texto
para a escrita dos \emph{scripts} em \emph{ChucK}: Vim e Emacs.
Renato utilizou aspectos estruturais de alto nivel, 
arcos longos e estruturas
melódicas e minimalistas, como os que seguem:

\code{Interface para controle de parâmetros em tempo
  real}{chuck_snippets/bar.ck}

Ao mesmo tempo, Vilson fez uso de scripts para
facilmente especificar a posição do arquivo de
áudio que deve ser iniciada a execução, assim como a velocidade
reprodução do áudio, duração e
amplitude. Uma interface simples foi permitiu a
alteração rápida desses parâmetros em tempo de execução.

\code{Interface para controle de parâmetros em tempo
  real}{chuck_snippets/foo.ck}

\code{Classe para \textit{sampling} de arquivos de
  áudio}{chuck_snippets/foosp.ck}

Esses desenvolvimentos todos inspiraram a criação de uma linguagem
específica para \emph{livecoding}, chamada \emph{Vivace}, que está em constante
desenvolvimento e tem seu código e maiores detalhes disponíveis online
de forma livre\footnote{Veja: 
\url{http://automata.github.com/vivace}.}. Já foram realizadas novas
apresentações utilizando essa linguagem, a notar: Hacklab do Velho, em
São Carlos (SP), Semana da Comunicação FAAP (SP), Palco UFSCar (SP) e
Semana Nacional da Ciência e Tecnologia (SP). Nelas houve a
predominância do apelo às mídias populares, em uma reinauguração do
gênero da \emph{pop art}: trechos de vídeos de uma novela brasileira: 'Avenida Brasil'
eram rearanjados em tempo real em sincronia com linhas rítmicas e
melódicas\footnote{Uma
  amostra está disponível \emph{online} para ser utilizada por qualquer
  em qualquer computador com o navegador \emph{Google-Chrome}:
  \url{http://pulapirata.com/skills/vivace}.}.

  Catalizadas pela aproximação de "Gera" Magela e Caleb Mascarenhas Luporini, as
   apresentações e amadurecimeentos de 2012 desembocaram na
  na escrita coletiva de códigos (acesso e controle da platéia 
  no código que está sendo escrito), utilização de monstros e ascii art e
  e sonoridades bizarras. Com isso, está sendo proposto
  um gênero de \emph{livecoding} batizado de '\emph{Freakcoding}', possivelmente o primeiro subgênero
  de \emph{livecoding}.

\subsubsection{ABeatTracker (ABT)}

O ABT é uma linguagem que dispara linhas rítmicas através
de macros. Nestas, especifica-se as células rítmicas, amostras sonoras que
são utilizadas como conteúdo sonoro destas linhas, modos de leitura
destas amostras sonoras e variáveis randômicas utilizadas para
execussão da linha. Além disso, o ABT dispõe de variáveis globais que
podem ser alteradas a qualquer momento pelo usuário também com macros, como BPM,
velocidade de leitura das amostras e variáveis randômicas globais (que
se somam às individuais).
As macros
pré-estabelecidas constituem um conjunto de recursos pré-estabelecidos
bem definido, o que contrasta com a ideia de uma 'linguagem de
programação' que tenha capacidades mais amplas. De qualquer forma,
linguagens com domínios específicos não são raras e por vezes o ABT
foi descrito como uma linguagem.

O ABT é um instrumento computacional essencialmente rítmico. Junto a esta
proposta, está a necessidade da utilização em conjunto com outros
instrumentos, externos ao ABT, ao computador em que o ABT está sendo executado
e possivelmente externo com relação a qualquer computador. Para isso
foi elaborado o ABD (ABeatDetector), no qual o usuário tamborila no
teclado do computador os rítmos que estiver ouvindo ou imaginando para
que o ABT sincronize o pulso e utilize células rítmicas
relacionadas. A análise feita pelo ABD resulta em uma série de rítmos
explicitados por sequências de compassos que encapsulem durações
regulares do rítmo tamborilado. Estes ritmos relacionados ao
tamborilar do usuário são chamados de harmônicos e podem ser
selecionados prontamente para o disparo de linhas melódicas no ABT.
De fato, atualmente o ABD é parte do ABT.

Todo o código do
ABT está disponivel online junto à documentação para uso\footnote{Veja o
repositório: git://labmacambira.git.sourceforge.net/labmacambira/audioArt/ABT}.


\subsection{Música na Matéria: EKP e AHT}
\label{aht}

Embora o foco deste trabalho seja na exploração musical através de
códigos computacionais, nesta sessão está explicação simples,
clara e factual de como estas investidas transcendem o código e até
mesmo a música em si.

Por vezes, mais relevantes que os próprios desenvolvimentos são as mobilizações
criadas nos entornos e os engajamentos.

A seguir estão dois trabalhos que geraram alguma mobilização,
resultando em trocas, reuniões, desenvolvimentos, pesquisas e
apresentações propriamente ditas. O primeiro utiliza o estado do
hardware como entrada, o segundo consiste na
flutuação de origamis para a obtenção de um instrumento musical
lúdico.


\subsubsection{Emotional Kernel Panic (EKP)}

Em 2008, em colaboração intensa com o CDTL (Centro de
Desenvolvimento de Tecnologias Livres)\footnote{Uma associação
  civil formada e desmembrada em 2008, sediada em Recife, PE.} foi
lançada a ideia de utilizar o estado do sistema operacional -
especialmente o kernel linux - para gerar de sons. Surge então o 
'\emph{Emotional Kernel Panic}' (EKP) na parceria de Felipe
Machado (então coordenador do CDTL),
Ricardo Brazileiro (artista conhecedor de tecnologias livres)
e o primeiro autor do presente trabalho.

Foram definidos três finalidades para esta exploração
do Sistema Operacional:

\begin{itemize}
\item Pedagógicos, através da utilização de um sentido que não o visual para a tarefa de analisar o Sistema.
\item Artísticos, para apresentações musicais/audiovisuais.
\item Monitoramento do SO, pela emissão de sons periódicos relativos à carga de processamento, memória, uso de rede, etc.
\end{itemize}

Esta empreitada se desdobrou em apresentações na Conferência Internacional de PureData (2009), SESC (2009),
e no III Festival Contato (2009). No Festival Internacional de Software Livre de 2010 foi apresentada
uma pesquisa sobre o EKP com diversos patches feitos por Ricardo Brazileiro e Felipe Machado, assim
como amadurecimentos da proposta.
Uma implementação conceitual do EKP está aqui: \url{http://trac.assembla.com/audioexperiments/browser/ekp-base}.



\subsubsection{AirHackTable}

A AirHackTable (AHT) é um instrumento musical eletrônico controlado por
origamis (dobraduras de papel), construída na forma de uma mesa. Nela,
uma rede de coolers reciclados faz flutuar origamis de geometria e
cores variadas. Os movimentos dos origamis são captados por \emph{webcam} e
interpretados em tempo real por software de processamento de imagens,
gerando padrões que controlam a transformação sonora da música. Dessa
forma, pode-se dizer que os sons gerados refletem o vôo dos origamis
de acordo com suas geometrias (que geram trajetórias de vôo
caracteristicas). Dito de maneira mais teórica, as estruturas fora
do tempo (geometrias dos origamis) estão sendo mapeadas em estruturas
temporais (trajetórias dos origamis em seus vôos), o que é um macete
da música erudita desde tempos antigos.

Na versão atual da AirHacktable, cada cor (vermelho, amarelo, azul,
verde, preto, ou branco) controla uma voz, já a posição do origami na
mesa controla aspectos do som. Como um exeplo, pode-se
configurar a AHT de forma que, se o origami está
flutuando para a esquerda, o som também se move para esquerda, se
o origami está mais próximo da câmera, o volume é maior, e se está
mais afastando do operador, o som se torna mais agudo.

A AHT segue a filosofia de desenvolvimento contínuo, comum ao software
livre, que atualmente foi incorporado ao movimento chamado hardware
livre ou aberto. A concepção inicial foi de Chico Simões (mestre de maracatu
e conhecedor de tecnologias livres) e o autor
deste texto, através do amadurecimento de possiveis instrumentos musicais de papel. 
Logo neste estágio inicial, foram aproximados Vilson Vieira, Ricardo Fabbri, Fábio Simões
e Daniel Penalva em reuniões presenciais. Destas reuniões, saíram ideias
diversas, escritas em um \emph{pad} aberto\footnote{Veja: \url{http://pontaopad.me/origami-sensores}.},
e, por fim, a AHT em si.

A primeira realização da mesa se deu no V Festival Contato, no \emph{Espaço Macambira},
com apresentação na Teia Casa de Criação, durante o mesmo festival. A estrutura
da mesa é também de papel (papelão, e foi feita
por Francisco Simões por pesquisa própria. O uso do reticulado de ventoinhas foi fruto
de discussões coletivas e de experimentos do coletivo labMacambira.sourceforge.net,
com especial empenho do Francisco e uso de sua luthearia de percussão: \emph{Tora Tambores}.

Em 2012 houve a aproximação de Caleb Mascarenhas Luporini e "Gera" Magela.
Foram feitas oficinas no SESC Pinheiros e SESC Belenzinho e apresentação
no AVAV\footnote{Evento mensal que ocorre na cidade São Paulo: AudioVisual Ao Vivo.},
todas com o foco na construção e uso da AHT.


\subsection{Música no tecido social: Sabrina Kawahara, Audioexperiments, EstudioLivre.org, CDTL, juntaDados.org, Devolts.org, MSST, LabMacambira.sf.net}

A singularidade do cruzamento artístico, cultural e tecnológico
dos trabalhos expostos também é desenvolvida por diferentes grupos.
Os propósitos de compartilhamento e apropriação tecnológica estão no
cerne destas comunidades, de forma que as investidas naturalmente tomam
rumos engajados socialmente. Em especial, o empoderamento civil e
a criação de um patrimônio tecnológico da humanidade são consequências
imediatas das posturas de compartilhamento e apropriação citados.

\begin{itemize}
    \item Sabrina Kawahara: formado pelo autor deste texto, Guilherme Lunhani,
    Guilherme Rebecchi, Israel Laurindo, Rodrigo Felício e Otávio Martigli,
    este grupo dedicou-se à realização
    de apresentações conjuntas, bem como a escrita de peças e textos através
    de dinâmicas coletivas.
    \item Audioexperiments (Æ): iniciado por Tiago Tavares, o autor do texto e aproximações de
    Daniel Pastore. O grupo se dedicou à publicação de textos didáticos e produção
    de código em repositórios públicos. Em destaque, o repositório no \emph{Assembla}
    possui códigos em Python e C/C++ que realizam diversos experimentos em áudio, música
    e inteligência artificial\footnote{Veja a página principal do espaço: \url{https://www.assembla.com/spaces/audioexperiments}.}.
    \item Estudios Livres: são ambientes de estudo, produção e distribuição de mídias livres.
    Formam, sem conjunto, um movimento e uma rede social brasileira, tida como única no mundo, embora
    o conceito seja conhecido internacionalmente\footnote{Há uma plataforma aberta com capacidades
    de \emph{wiki}, blog e midiateca em \url{http://estudiolivre.org}.}. Tanto a comunidade quanto
    a lista operacional e plataforma da comunidade foram essenciais nos amadurecimentos de questões
    relacionadas às mídias livres e ao uso de Python para áudio e música, como pode ser visto
    na Carta Mídias Livres e no Tutorial de Python para Áudio e Música, abordados abaixo
    em~\ref{subsec:tuts} deste mesmo Apêndice.
    \item CDTL (Centro de Desenvolvimento de Tecnologias Livres): foi uma associação civil com base em Recife/PE, dedicada ao desenvolvimento e difusão de tecnologias livres. Esta associação proporcionou suporte para os primeiros materiais didáticos e investidas em plugins de áudio através do convênio estabelecido com o Ministério da Cultura de Pontão de Cultura Digital.
    \item JuntaDados.org: é um grupo com membros em todo o território brasileiro e ainda ativo. Embora tenha havido um momento de maior institucionalidade, durante o convênio de Pontão de Cultura Digital com o Ministério da Cultura e parceria com a Universidade Estadual da Bahia (UNEB), o grupo é marcado pela descentralização e autonomia dos envolvidos. O autor é membro fundador e ativo do juntaDados, em conjunto, especialmente, com Fabiana "Goa" Sherine e Marcelo Soares Souza. Neste grupo foram desenvolvidas ideias como o EKP, o ABT, os tutoriais de Python para Áudio e escritos socialmente engajados.
    \item Devolts.org: responsável por Esporos de Pesquisa e Experimentação do programa Cultura Viva, os "Jardins Devolts" cultivam listas nacionais de trocas de conhecimentos livres e cultivo de informações voltadas para música com usos especiais de \emph{PureData}, \emph{Arduinos} e \emph{Python}. Este é um meio de troca rico e alternativo a alguns meios mais politizados, como a lista do \emph{estudiolivre.org}.
    \item MSST (Movimento dos Sem Satélite): com um manifesto e propostas coerentes, mas nada óbvias, como a demanda específica por satélites civís, abordagens tecnomágicas, o grupo é uma instâncias iniciáticas em termos tecnológicos e conceituais.
    \item LabMacambira.sf.net: espaço virtual de programadores dedicados ao audiovisual e propósitos socialmente engajados. Fundado em parceria do autor deste trabalho e de Vilson Vieira, Daniel Marostegan e Carneiro e Ricardo Fabbri e de parte dos investimentos da segunda etapa do convênio de Pontão de Cultura Digital Nós Digitais, da Teia Casa de Criação. Em um primeiro momento, membros foram remunerados como mentores e como aprendizes\footnote{Renato Fabbri, Ricardo Fabbri, Vilson Vieira, Marcos Mendonça e Gilson Beck mentoraram; Alexandre "Bzum" Negrão, Lucas Zambianchi, Larissa Arruda, Nivaldo Henrique Bondança, Fernando Gorodscy, Andres Martano desenvolveram como bolsistas; Danilo Shiga e Marcos Murad foram parceiros colaboradores; Francisco Simões, Fábio Simões, Caleb Mascarenhas Luporini, "Gera" Magela, Edson "Presto" Correia, Daniel Penalva se aproximaram para atividades específicas, cruciais em todo ano já sem os investimentos do convênio.}. Em um segundo momento, aproximações e parcerias diversas ocorreram. Detalhes estão mas subseções seguintes de \emph{Web} e materiais didáticos.
    \item Submidialogia: grupo bastante relacionado aos encontros de '\emph{submidialogias}, ao Descentro e às ideias de submídia. A lista de emails do grupo possui membros de diversos dos outros grupos citados.
    \item Outros grupos relacionados: o MuSa foi um grupo de JoinVille importante neste trabalho por ter sido fundado por Vilson Vieira, que possui presença forte no LabMacambira.sf.net e nos trabalhos aqui descritos. O  Metareciclagem é uma rede engajada em apropriação tecnológica e empoderamento civil com bastante presença dos outros grupos aqui citados e muito movimentada. A Casa de Cultura Tainã é uma entidade cultural de Campinas/SP, referência no uso de tecnologias livres e atividades socialmente engajadas, e onde muito do que está aqui descrito foi inspirado.
\end{itemize}



\section{Materiais didáticos}

  \subsection{ Tutoriais em texto e código: python, filtros, nyquist e plugins LADSPA, metrics, carta mídias livres, contra-cultura digital }\label{subsec:tuts}

\begin{itemize}
    \item {\bf Tutorial de python para áudio e som}

Este tutorial foi apresentado parcialmente em Berlim no LAC 2007 e, desde então, foi melhorado algumas vezes. Esta
primeira versao ficou resumida em forma de texto na plataforma
Estúdio Livre\footnote{\url{http://estudiolivre.org/python-e-som-tutorial}.}.
Um agradecimento especial para Fábio Furlanete e Marília Chiozo pelas contribuições.
Em 2010
a Associacao Python Brasil escolheu este trabalho, então já mais maduro, para ser apresentado no
Festival Internacional de \emph{Software} Livre, em Porto Alegre. Como consequência, 
foi feita uma série de video-tutoriais\footnote{http://estudiolivre.org/tiki-index.php?page=Video+Tutoriais}.
Os vídeos são tidos como iniciáticos em Python por diferentes pessoas ligadas ao movimento de \emph{software} livre.

    \item {\bf Tutoriais de filtros e amostragem via python }

Voltados para explicitar fundamentos de áudio digital, estes tutoriais
são baseados código Python e o equivalente em C. Pequenas explicações são
dadas com o intuito de orientar a exploração inteligente destes \emph{snippets}.
\emph{Teorema de Amostragem}: estes \emph{scripts} executam experimentações ilustrativas com
o Teorema de Nyquist e figuras.
\emph{Filtros}: além de filtros FIR e IIR simples,
duas utilizações clássicas destes filtros estão implementadas
de forma didática: Wavelets (FIR) e Quad (IIR).

Estes códigos didáticos podem ser baixados do repositório \emph{AudioExperiments}.

    \item {\bf Tutorial de plugins lv2}

Dadas as dificuldades que o desenvolvimento dos \emph{plugins} de áudio apresenta,
foi feito um tutorial passo a passo com plugins que rodam em todas as etapas.
Ele é baseado em uma interface C++ para este padrão de plugin que eh implementado
em C. Os códigos e os textos estão todos em repositório.

    \item {\bf Microtutoriais Django ~\cite{dmicrotuts}}

Estes 'microtutoriais' são baseados nos conceitos de \emph{scripts mínimos} e
\emph{alterações puntuais}. O primeiro conjunto de microtutoriais é dedicado
a reconstruir o tutorial oficial do django de forma condensada e não prolixa.
O segundo destes conjuntos é dedicado a instrumentalizar de fato o leitor com
o entendimento do funcionamento dos princípios fundamentais deste framework.
Uma das primeiras incidências dos \emph{scripts} mínimos, muito usados no
labMacambira.sourceforge.net para passagens de tecnologias de forma precisa.

     \item {\bf Figusdevpack (FDP)}

Idealizado como um meio de interação da comunidade de Python e Música 
para compartilhamento de códigos e excertos, o FDP foi Baseado principalmente
em documentação organizada sobre as práticas e as bibliotecas
existentes para Python. Parte desta documentação
é proposta como \emph{scripts} que
fazem uso de objetos e módulos de forma isolada, puntual.
 Este trabalho foi aceito na
maior conferência de áudio em linux, a Linux Audio Conference de 2008
(LAC2008) e foi reativado algumas vezes pelo autor deste texto
em conjunto com Vilson Vieira, Ivan Marin
e outros desenvolvedores. Este projeto está
documentado na plataforma do Estúdio Livre\footnote{
Veja a página do PDF \url{http://estudiolivre.org/tiki-index.php?page=fdp&highlight=fdp}, o artigo aceito no LAC2008 \url{http://www.estudiolivre.org/el-gallery_view.php?arquivoId=8221}
e o repositório do FDP \url{http://sourceforge.net/projects/fdpack/develop fdpsf}.}.


    \item {\bf Carta mídias livres}

Texto criado em decorrência da participação da comissão de seleção no
'Prêmio Mídias Livres', a convite do Ministério da Cultura por 'notório saber'. A participação consistia em analisar os inscritos no Prêmio Mídias Livres e distribuir 4 milhões de reais dentre categorias regionais e nacionais.
Esta carta é um documento único, deixando às claras
o conceito de Mídias Livres como não aprisionadas pelo conceito
de propriedade, ou seja, que priorizam a sua livre circulação e a possibilidade
de geração de materiais derivados. Há o viés de priorizar processos colaborativos, comunitários e setores da sociedade menos contemplados na geração e circulação midiática\footnote{Veja: \url{http://www.estudiolivre.org/carta-pts-midias-livres}}.

    \item {\bf Philosometrics}

Em decorrência deste trabalho, surgiu o  Musimetrics,
o Cinemetrics e o Literametrics. Uma publicação no
JSTAT condensa e compara as análises de filosofia
e música.\cite{FabbriJSTAT}
Além disso, ele é uma
utilização das ciências duras para 
assunto mais incidente em ciências humanas. 

    \item {\bf Textos socialmente engajados }

O uso de pseudônimos é um costume apreciado em diversos meios. As pesquisas informais confirmam vantagens desta prática\footnote{Veja: \url{http://disqus.com/research/pseudonyms/}}.
Em especial utiliza-se pseudônimos para
auxiliar a despersonificação, gerando textos menos presos à satisfação da auto-imagem. Como resultado, além de aumento de produtividade, constuma-se conseguir também compreensões diferentes. Destes textos, dois foram publicados em duas publicações relevantes: O Contracultura Digital e o Peixe Morto, este último relacionado aos submidialogias\footnote{Veja: \url{http://mutgamb.org/blog/Submidialogias-Peixe-Morto-para-Baixar} e \url{http://culturadigital.br/contraculturadigital/2012/02/01/publicacao-contraculturadigital/}.}.

 \end{itemize}

\subsection{Screencasts e outros materiais em video}

\begin{itemize}
    \item Python para áudio e música
    Relacionado ao tutorial em texto citado acima, foram feitos videos sobre

    \item Canal Macambira
No Macambira estão sendo produzidos materiais em screencasts sobre
diversas cenas de hackeamento.

    \begin{itemize}
	\item Live-Coding
	\item Raspagem de dados
    \end{itemize}
\end{itemize}





\section{Web}
  Difusão de informação com ênfase na facilitação
  da apropriação de tecnologias e de instancias políticas.

   \subsection{Tecnologias sociais de alta demanda: Sitios, Conteúdos e Articulação}

      \subsubsection{Sítios}

      FDDCA

      Ferramenta de comunicação

      (Cadastro dos pontos?)

      AA, SOS, Catalogo de Ideias, etc

      Meu site pessoal


      \subsubsection{Conteúdos}

      Wiki?

      \subsubsection{Articulação}

      IRC, Emails

\subsection{Disponibilização e desenvolvimento conjunto: wikis, etherpads, AA, Trac, IRC ..}

\subsubsection{Wiki}

\subsubsection{Trac}

\subsubsection{Screencasts - Vimeo}

\subsubsection{AA}

\subsubsection{Audio Experiments (Æ)}

\subsubsection{IRC}

\subsubsection{Etherpads}

\subsubsection{Outras fontes}


